{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPU Multi Mirrored Strategy .ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMB/jCMny+OqJ5WhIw9SGI/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prikshit-2000/Tensorflow/blob/main/GPU_Multi_Mirrored_Strategy_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "co2h0vSTD5zt"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwRdO1vNHfXw",
        "outputId": "f1bb716e-f97a-48fb-f349-f13ab1662edb"
      },
      "source": [
        "os.environ['TF_MIN_GPU_MULTIPROCESSOR_COUNT'] = '4'\n",
        "strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\n",
        "print(\"Num of devices : {}\".format(strategy.num_replicas_in_sync))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "Num of devices : 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyfjXfhZH-jk"
      },
      "source": [
        "# Get the data\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# Adding a dimension to the array -> new shape == (28, 28, 1)\n",
        "# We are doing this because the first layer in our model is a convolutional\n",
        "# layer and it requires a 4D input (batch_size, height, width, channels).\n",
        "# batch_size dimension will be added later on.\n",
        "train_images = train_images[..., None]\n",
        "test_images = test_images[..., None]\n",
        "\n",
        "# Normalize the images to [0, 1] range.\n",
        "train_images = train_images / np.float32(255)\n",
        "test_images = test_images / np.float32(255)\n",
        "\n",
        "# Batch the input data\n",
        "BUFFER_SIZE = len(train_images)\n",
        "BATCH_SIZE_PER_REPLICA = 64\n",
        "GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
        "\n",
        "# Create Datasets from the batches\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(BUFFER_SIZE).batch(GLOBAL_BATCH_SIZE)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(GLOBAL_BATCH_SIZE)\n",
        "\n",
        "# Create Distributed Datasets from the datasets\n",
        "train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)\n",
        "test_dist_dataset = strategy.experimental_distribute_dataset(test_dataset)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7sTgCwBIK9a"
      },
      "source": [
        "# Create the model architecture\n",
        "def create_model():\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
        "      tf.keras.layers.MaxPooling2D(),\n",
        "      tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
        "      tf.keras.layers.MaxPooling2D(),\n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dense(64, activation='relu'),\n",
        "      tf.keras.layers.Dense(10)\n",
        "    ])\n",
        "  return model"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74A_pkVjIrmy",
        "outputId": "5f04d8b3-6241-4b5c-ba2e-a50a899e46c7"
      },
      "source": [
        "with strategy.scope():\n",
        "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,reduction = tf.keras.losses.Reduction.NONE)\n",
        "\n",
        "  def compute_loss(labels,predictions):\n",
        "\n",
        "    per_example_loss = loss_object(labels,predictions)\n",
        "\n",
        "    print(per_example_loss)\n",
        "    return tf.nn.compute_average_loss(per_example_loss , global_batch_size=GLOBAL_BATCH_SIZE)\n",
        "\n",
        "  test_loss = tf.keras.metrics.Mean(name = 'test_loss')\n",
        "\n",
        "  train_accuracy = tf.keras.metrics.SparseTopKCategoricalAccuracy()\n",
        "  test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "  optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "  model  = create_model()\n",
        "  \n",
        "\n",
        "  "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptwP0WRlJ2iH"
      },
      "source": [
        "@tf.function\n",
        "def distributed_train_step(dataset_inputs):\n",
        "  per_replica_loss = strategy.run(train_step , args = (dataset_inputs , ))\n",
        "  return strategy.reduce(tf.distribute.ReduceOp.SUM,per_replica_loss,axis = None)\n",
        "\n",
        "\n",
        "def train_step(inputs):\n",
        "  images,labels = inputs\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(images , training = True)\n",
        "    loss = compute_loss(labels,predictions)\n",
        "\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "  train_accuracy.update_state(labels,predictions)\n",
        "  return loss\n",
        "\n",
        "\n",
        "@tf.function\n",
        "\n",
        "def distributed_test_step(dataset_inputs):\n",
        "  return strategy.run(test_step , args = (dataset_inputs , ))\n",
        "\n",
        "def test_step(inputs):\n",
        "  images,labels = inputs\n",
        "  predictions = model(images, training=False)\n",
        "  t_loss = loss_object(labels, predictions)\n",
        "\n",
        "  test_loss.update_state(t_loss)\n",
        "  test_accuracy.update_state(labels, predictions)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkyCKOKdLDNT",
        "outputId": "c38a1efd-2adf-43a5-ce5d-cd2e15ede4db"
      },
      "source": [
        "EPOCHS = 10\n",
        "for epoch in range(EPOCHS):\n",
        "  # Do Training\n",
        "  total_loss = 0.0\n",
        "  num_batches = 0\n",
        "  for batch in train_dist_dataset:\n",
        "    total_loss += distributed_train_step(batch)\n",
        "    num_batches += 1\n",
        "  train_loss = total_loss / num_batches\n",
        "\n",
        "  # Do Testing\n",
        "  for batch in test_dist_dataset:\n",
        "    distributed_test_step(batch)\n",
        "\n",
        "  template = (\"Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, \" \"Test Accuracy: {}\")\n",
        "\n",
        "  print (template.format(epoch+1, train_loss, train_accuracy.result()*100, test_loss.result(), test_accuracy.result()*100))\n",
        "\n",
        "  test_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  test_accuracy.reset_states()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor(\"sparse_categorical_crossentropy/weighted_loss/Mul:0\", shape=(64,), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
            "Tensor(\"sparse_categorical_crossentropy/weighted_loss/Mul:0\", shape=(32,), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
            "Epoch 1, Loss: 0.2855414152145386, Accuracy: 99.64778137207031, Test Loss: 0.30135786533355713, Test Accuracy: 89.25\n",
            "Epoch 2, Loss: 0.2560100257396698, Accuracy: 99.90333557128906, Test Loss: 0.30213630199432373, Test Accuracy: 89.11000061035156\n",
            "Epoch 3, Loss: 0.23390589654445648, Accuracy: 99.94166564941406, Test Loss: 0.2914637625217438, Test Accuracy: 89.45000457763672\n",
            "Epoch 4, Loss: 0.21510154008865356, Accuracy: 99.95166778564453, Test Loss: 0.2709631621837616, Test Accuracy: 90.34000396728516\n",
            "Epoch 5, Loss: 0.19550323486328125, Accuracy: 99.961669921875, Test Loss: 0.2542589604854584, Test Accuracy: 91.16999816894531\n",
            "Epoch 6, Loss: 0.17982305586338043, Accuracy: 99.97000122070312, Test Loss: 0.25812649726867676, Test Accuracy: 90.93000030517578\n",
            "Epoch 7, Loss: 0.16500601172447205, Accuracy: 99.97000122070312, Test Loss: 0.2568337023258209, Test Accuracy: 90.97000122070312\n",
            "Epoch 8, Loss: 0.14981874823570251, Accuracy: 99.9749984741211, Test Loss: 0.2800830006599426, Test Accuracy: 90.55999755859375\n",
            "Epoch 9, Loss: 0.14200203120708466, Accuracy: 99.99166870117188, Test Loss: 0.28930142521858215, Test Accuracy: 90.73999786376953\n",
            "Epoch 10, Loss: 0.12786108255386353, Accuracy: 99.98999786376953, Test Loss: 0.28309351205825806, Test Accuracy: 90.8699951171875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_sveDaTLKRg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}